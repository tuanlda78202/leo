{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd6eadfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"tuanlda78202/leo_summarization_task\"\n",
    "model_name = \"tuanlda78202/Qwen3-1.7B-Leo-Summarization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e7343f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-30 03:30:10 [__init__.py:243] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.9.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.638 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2048, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=6144, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=6144, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=6144, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=40960,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fccc4511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(dataset_name, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93542646",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are a helpful assistant specialized in summarizing documents. Generate a concise TL;DR summary in markdown format having a maximum of 512 characters of the key findings from the provided documents, highlighting the most significant insights\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "886adfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "\n",
    "def generate_text(\n",
    "    instruction, streaming: bool = True, trim_input_message: bool = False\n",
    "):\n",
    "    message = alpaca_prompt.format(\n",
    "        instruction,\n",
    "        \"\",\n",
    "    )\n",
    "    inputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    if streaming:\n",
    "        return model.generate(\n",
    "            **inputs, streamer=text_streamer, max_new_tokens=256, use_cache=True\n",
    "        )\n",
    "    else:\n",
    "        output_tokens = model.generate(**inputs, max_new_tokens=256, use_cache=True)\n",
    "        output = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "        if trim_input_message:\n",
    "            return output[len(message) :]\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e8a3da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "You are a helpful assistant specialized in summarizing documents. Generate a concise TL;DR summary in markdown format having a maximum of 512 characters of the key findings from the provided documents, highlighting the most significant insights\n",
      "\n",
      "### Input:\n",
      "![preloader](https://ploomber.io/images/favicon_hu25b163a2750ceab2fb56e04dc19e17ac_2757_36x0_resize_q90_h2_box_2.webp)\n",
      "[![Ploomber](https://ploomber.io/images/logo.png) Ploomber](https://ploomber.io/)\n",
      "  * [Home](https://ploomber.io/)\n",
      "  * [Blog](https://ploomber.io/blog/)\n",
      "  * [Pricing](https://ploomber.io/pricing/)\n",
      "  * [AI Editor](https://editor.ploomber.io/)\n",
      "  * [Docs](https://docs.cloud.ploomber.io)\n",
      "  * [Contact](https://ploomber.io/contact/)\n",
      "  * [Explore](https://ploomber.io/blog/vllm-deploy/)\n",
      "    * [Sample Apps](https://www.platform.ploomber.io/explore)\n",
      "    * [Streamlit with Microsoft Entra ID](https://ploomber.io/streamlit-microsoft-entra/)\n",
      "    * [Shiny with Microsoft Entra ID](https://ploomber.io/shiny-microsoft-entra/)\n",
      "    * [Dash with Microsoft Entra ID](https://ploomber.io/dash-microsoft-entra/)\n",
      "    * [Streamlit with Google OAuth](https://ploomber.io/streamlit-google-auth/)\n",
      "    * [Shiny with Google OAuth](https://ploomber.io/shiny-google-auth/)\n",
      "    * [Dash with Google OAuth](https://ploomber.io/dash-google-auth/)\n",
      "    * [Streamlit Enterprise](https://ploomber.io/streamlit-enterprise/)\n",
      "    * [Shiny Enterprise](https://ploomber.io/shiny-enterprise/)\n",
      "    * [Dash Enterprise](https://ploomber.io/dash-enterprise/)\n",
      "\n",
      "\n",
      "[Get Started](https://www.platform.ploomber.io/register?utm_medium=website&utm_source=top-button)\n",
      "# Deploying vLLM: a Step-by-Step Guide\n",
      "![author image](https://ploomber.io/images/author/eduardo_hu9e19b4a2aaddfa8a3701db9c795afd57_8621_5a035ab53cde9a40f0a262203a70a108.webp)\n",
      "Eduardo Blancas\n",
      "Mar 28, 2024 - 11 Min read\n",
      "# Table of Contents\n",
      "  *     * [Getting started with vLLM](https://ploomber.io/blog/vllm-deploy/#getting-started-with-vllm)\n",
      "    * [Installing vLLM](https://ploomber.io/blog/vllm-deploy/#installing-vllm)\n",
      "    * [Checking your installation](https://ploomber.io/blog/vllm-deploy/#checking-your-installation)\n",
      "    * [Starting the vLLM server](https://ploomber.io/blog/vllm-deploy/#starting-the-vllm-server)\n",
      "      * [Setting the `dtype`](https://ploomber.io/blog/vllm-deploy/#setting-the-dtype)\n",
      "    * [Making requests](https://ploomber.io/blog/vllm-deploy/#making-requests)\n",
      "      * [Using the OpenAI client](https://ploomber.io/blog/vllm-deploy/#using-the-openai-client)\n",
      "    * [Using the chat API](https://ploomber.io/blog/vllm-deploy/#using-the-chat-api)\n",
      "      * [Security settings](https://ploomber.io/blog/vllm-deploy/#security-settings)\n",
      "    * [Considerations for a production deployment](https://ploomber.io/blog/vllm-deploy/#considerations-for-a-production-deployment)\n",
      "    * [Using PyTorchâ€™s docker image](https://ploomber.io/blog/vllm-deploy/#using-pytorchs-docker-image)\n",
      "      * [Cautionary tale about a bug in the `transformers==4.39.1` package](https://ploomber.io/blog/vllm-deploy/#cautionary-tale-about-a-bug-in-the-transformers4391-package)\n",
      "    * [Deploying on Ploomber Cloud](https://ploomber.io/blog/vllm-deploy/#deploying-on-ploomber-cloud)\n",
      "\n",
      "\n",
      "Deploy **a vLLM server in a few clicks** with Ploomber\n",
      "[Start for free](https://www.platform.ploomber.io/register/?utm_source=vllm-deploy&utm_medium=blog)\n",
      "Need help? Join our [community!](https://www.ploomber.io/community)\n",
      "## Table of Contents\n",
      "  *     * [Getting started with vLLM](https://ploomber.io/blog/vllm-deploy/#getting-started-with-vllm)\n",
      "    * [Installing vLLM](https://ploomber.io/blog/vllm-deploy/#installing-vllm)\n",
      "    * [Checking your installation](https://ploomber.io/blog/vllm-deploy/#checking-your-installation)\n",
      "    * [Starting the vLLM server](https://ploomber.io/blog/vllm-deploy/#starting-the-vllm-server)\n",
      "      * [Setting the `dtype`](https://ploomber.io/blog/vllm-deploy/#setting-the-dtype)\n",
      "    * [Making requests](https://ploomber.io/blog/vllm-deploy/#making-requests)\n",
      "      * [Using the OpenAI client](https://ploomber.io/blog/vllm-deploy/#using-the-openai-client)\n",
      "    * [Using the chat API](https://ploomber.io/blog/vllm-deploy/#using-the-chat-api)\n",
      "      * [Security settings](https://ploomber.io/blog/vllm-deploy/#security-settings)\n",
      "    * [Considerations for a production deployment](https://ploomber.io/blog/vllm-deploy/#considerations-for-a-production-deployment)\n",
      "    * [Using PyTorchâ€™s docker image](https://ploomber.io/blog/vllm-deploy/#using-pytorchs-docker-image)\n",
      "      * [Cautionary tale about a bug in the `transformers==4.39.1` package](https://ploomber.io/blog/vllm-deploy/#cautionary-tale-about-a-bug-in-the-transformers4391-package)\n",
      "    * [Deploying on Ploomber Cloud](https://ploomber.io/blog/vllm-deploy/#deploying-on-ploomber-cloud)\n",
      "\n",
      "\n",
      "Deploy **a vLLM server in a few clicks** with Ploomber\n",
      "[Start for free](https://www.platform.ploomber.io/register/?utm_source=vllm-deploy&utm_medium=blog)\n",
      "[vLLM](https://docs.vllm.ai/en/latest/) is one of the most exciting LLM projects today. With over [200k monthly downloads](https://pypistats.org/packages/vllm), and a permissive Apache 2.0 License, vLLM is becoming an increasingly popular way to serve LLMs at scale.\n",
      "In this tutorial, Iâ€™ll show you how you can configure and run vLLM to serve open-source LLMs in production.\n",
      "## Getting started with vLLM\n",
      "For those new to vLLM, letâ€™s first explain what vLLM is.\n",
      "vLLM is an open-source project that allows you to do LLM inference and serving. Inference means that you can download model weights and pass them to vLLM to perform inference via their Python API; hereâ€™s an example from their documentation:\n",
      "Copy```\n",
      "from vllm import LLM, SamplingParams\n",
      "prompts = [\n",
      "  \"Hello, my name is\",\n",
      "  \"The president of the United States is\",\n",
      "  \"The capital of France is\",\n",
      "  \"The future of AI is\",\n",
      "]\n",
      "# initialize\n",
      "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
      "llm = LLM(model=\"facebook/opt-125m\")\n",
      "# perform the inference\n",
      "outputs = llm.generate(prompts, sampling_params)\n",
      "# print outputs\n",
      "for output in outputs:\n",
      "  prompt = output.prompt\n",
      "  generated_text = output.outputs[0].text\n",
      "  print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
      "\n",
      "```\n",
      "\n",
      "In this regard, vLLM is similar to Hugginfaceâ€™s [transformers](https://github.com/huggingface/transformers) library, as a comparison, hereâ€™s how you do inference on the same model using transformers:\n",
      "Copy```\n",
      "from transformers import pipeline\n",
      "generator = pipeline('text-generation', model=\"facebook/opt-125m\")\n",
      "generator(\"Hello, my name is\")\n",
      "\n",
      "```\n",
      "\n",
      "Running inference using the Python API, as I showed in the previous example, is fine for quick testing, but in a production setting, we want to offer a simple interface to interact with the model so other parts of the system can call it easily, a great solution is to expose our model via an API.\n",
      "Letâ€™s say you found out about vLLM, and now you want to build a REST API to serve a model, you might build a Flask app like this:\n",
      "Copy```\n",
      "from flask import Flask, request, jsonify\n",
      "from vllm import LLM, SamplingParams\n",
      "app = Flask(__name__)\n",
      "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
      "llm = LLM(model=\"facebook/opt-125m\")\n",
      "@app.route('/generate', methods=['POST'])\n",
      "def generate():\n",
      "  data = request.get_json()\n",
      "  prompts = data.get('prompts', [])\n",
      "  outputs = llm.generate(prompts, sampling_params)\n",
      "  # Prepare the outputs.\n",
      "  results = []\n",
      "  for output in outputs:\n",
      "    prompt = output.prompt\n",
      "    generated_text = output.outputs[0].text\n",
      "    results.append({\n",
      "      'prompt': prompt,\n",
      "      'generated_text': generated_text\n",
      "    })\n",
      "  return jsonify(results)\n",
      "if __name__ == '__main__':\n",
      "  app.run(host='0.0.0.0', port=5000)\n",
      "\n",
      "```\n",
      "\n",
      "Our users can now consume our model by hitting the `/generate` endpoint. However, this has many limitations: if many users hit the endpoint simultaneously, Flask will attempt to run them concurrently and crash. We also need to implement our authentication mechanism. Finally, interoperability is limited; users must read our modelâ€™s REST API documentation to interact with our model.\n",
      "This is where the serving part of vLLM shines since it provides all of this for us. If vLLMâ€™s Python API is akin to the transformers library, vLLMâ€™s server is akin to [TGI](https://github.com/huggingface/text-generation-inference).\n",
      "Now that we have explained the basics of vLLM; letâ€™s install it!\n",
      "## Installing vLLM\n",
      "Installing vLLM is simple:\n",
      "Copy```\n",
      "pip install vllm\n",
      "\n",
      "```\n",
      "\n",
      "Keep in mind that vLLM requires Linux and Python >=3.8. Furthermore, it requires a GPU with compute capability >=7.0 (e.g., V100, T4, RTX20xx, A100, L4, H100).\n",
      "Finally, vLLM is compiled with CUDA 12.1, so you need to ensure that your machine is running such CUDA version. To check it, run:\n",
      "Copy```\n",
      "nvcc --version\n",
      "\n",
      "```\n",
      "\n",
      "If youâ€™re not running CUDA 12.1 you can either install a version of vLLM compiled with the CUDA version youâ€™re running (see the [installation instructions](https://docs.vllm.ai/en/latest/getting_started/installation.html) to learn more), or install CUDA 12.1.\n",
      "## Checking your installation\n",
      "Before continuing, Iâ€™d advise you to check your installation by running some sanity checks:\n",
      "Copy```\n",
      "# ensure torch is working with CUDA, this should print: True\n",
      "python -c 'import torch; print(torch.cuda.is_available())'\n",
      "\n",
      "```\n",
      "\n",
      "Now, store the following in a `check-vllm.py` file:\n",
      "Copy```\n",
      "from vllm import LLM, SamplingParams\n",
      "prompts = [\n",
      "  \"Mexico is famous for \",\n",
      "  \"The largest country in the world is \",\n",
      "]\n",
      "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
      "llm = LLM(model=\"facebook/opt-125m\")\n",
      "responses = llm.generate(prompts, sampling_params)\n",
      "for response in responses:\n",
      "  print(response.outputs[0].text)\n",
      "\n",
      "```\n",
      "\n",
      "And run the script:\n",
      "Copy```\n",
      "python check-vllm.py\n",
      "\n",
      "```\n",
      "\n",
      "After the model is loaded, youâ€™ll see some output; in my case, I got this:\n",
      "Copy```\n",
      "~~national~~ cultural and artistic art. They've already worked with him.\n",
      "~~the country~~ a capitalist system with the highest GDP per capita in the world\n",
      "\n",
      "```\n",
      "\n",
      "## Starting the vLLM server\n",
      "Now that we have vLLM installed, letâ€™s start the server. The basic command is as follows:\n",
      "Copy```\n",
      "python -m vllm.entrypoints.openai.api_server --model=MODELTORUN\n",
      "\n",
      "```\n",
      "\n",
      "Where `MODELTORUN` is the [model you want to serve](https://docs.vllm.ai/en/latest/models/supported_models.html), for example, to serve `google/gemma-2b`.\n",
      "Copy```\n",
      "python -m vllm.entrypoints.openai.api_server --model=google/gemma-2b\n",
      "\n",
      "```\n",
      "\n",
      "Note that some models, such as `google/gemma-2b` require you to accept their license, hence, you need to create a HuggingFace account, accept the modelâ€™s license, and generate a token.\n",
      "For example, when [opening `google/gemma-2b` on HugginFace](https://huggingface.co/google/gemma-2b) (you need to be logged in), youâ€™ll see this:\n",
      "![accept gemma license](https://ploomber.io/images/blog/vllm-deploy/gemma-license.png)\n",
      "Once you accept the license, head over to the [tokens section](https://huggingface.co/settings/tokens), and grab a token, then, before starting vLLM, set the token as follows:\n",
      "Copy```\n",
      "export HF_TOKEN=YOURTOKEN\n",
      "\n",
      "```\n",
      "\n",
      "Once the token is set, you can start the server.\n",
      "Copy```\n",
      "python -m vllm.entrypoints.openai.api_server --model=google/gemma-2b\n",
      "\n",
      "```\n",
      "\n",
      "Note that the token is required even if you downloaded the weights. Otherwise youâ€™ll get the following error:\n",
      "Copy```\n",
      " File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 863, in _raise_file_not_found\n",
      "  raise FileNotFoundError(msg) from err\n",
      "FileNotFoundError: google/gemma-2b (repository not found)\n",
      "\n",
      "```\n",
      "\n",
      "### Setting the `dtype`\n",
      "One important setting to consider is `dtype`, which controls the data type for the model weights. You might need to tweak this parameter depending on your GPU, for example, trying to run `google/gemma-2b`:\n",
      "Copy```\n",
      "# --dtype=auto is the default value\n",
      "python -m vllm.entrypoints.openai.api_server --model=google/gemma-2b --dtype=auto\n",
      "\n",
      "```\n",
      "\n",
      "On an NVIDIA Tesla T4 yields the following error:\n",
      "Copy```\n",
      "ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0.\n",
      "Your Tesla T4 GPU has compute capability 7.5. You can use float16 instead by explicitly\n",
      "setting the`dtype` flag in CLI, for example: --dtype=half.\n",
      "\n",
      "```\n",
      "\n",
      "Changing the `--dtype` flag allows us to run the model on a T4:\n",
      "Copy```\n",
      "python -m vllm.entrypoints.openai.api_server --model=google/gemma-2b --dtype=half\n",
      "\n",
      "```\n",
      "\n",
      "If this is the first time you start vLLM with the passed `--model`, itâ€™ll take a few minutes since it has to download the weights. Further initializations will be faster since weights are stored in the `~/.cache` directory; however, since the model has to load into memory, itâ€™ll still take some time to load (depending on the model size).\n",
      "If you see a message like this:\n",
      "Copy```\n",
      "INFO:   Started server process [428]\n",
      "INFO:   Waiting for application startup.\n",
      "INFO:   Application startup complete.\n",
      "INFO:   Uvicorn running on http://0.0.0.0:80 (Press CTRL+C to quit)\n",
      "\n",
      "```\n",
      "\n",
      "vLLM is ready to accept requests!\n",
      "## Making requests\n",
      "Once the server is running, you can make requests; hereâ€™s an example using `google/gemma-2b` and the Python `requests` library:\n",
      "Copy```\n",
      "# remember to run: pip install requests\n",
      "import requests\n",
      "import json\n",
      "# change for your host\n",
      "VLLM_HOST = \"https://autumn-snow-1380.ploomber.app\"\n",
      "url = f\"{VLLM_HOST}/v1/completions\"\n",
      "headers = {\"Content-Type\": \"application/json\"}\n",
      "data = {\n",
      "  \"model\": \"google/gemma-2b\",\n",
      "  \"prompt\": \"JupySQL is\",\n",
      "  \"max_tokens\": 100,\n",
      "  \"temperature\": 0\n",
      "}\n",
      "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
      "print(response.json()[\"choices\"][0][\"text\"])\n",
      "\n",
      "```\n",
      "\n",
      "This is the response that I got:\n",
      "Copy```\n",
      "JupySQL is a Python library that allows you to create and run SQL queries in Jupyter notebooks. It is a powerful tool for data analysis and visualization, and can be used to explore and manipulate large datasets.\n",
      "How does JupySQL work?\n",
      "JupySQL works by connecting to a database server and executing SQL queries. It supports a wide range of databases, including MySQL, PostgreSQL, and SQLite.\n",
      "Once you have connected to a database, you can create and run SQL queries in\n",
      "\n",
      "```\n",
      "\n",
      "[Accurate!](https://github.com/ploomber/jupysql)\n",
      "### Using the OpenAI client\n",
      "vLLM exposes an API that mimics OpenAIâ€™s one; which implies that you can use OpenAIâ€™s Python package but direct calls to your vLLM server. Letâ€™s see an example:\n",
      "Copy```\n",
      "# NOTE: remember to run: pip install openai\n",
      "from openai import OpenAI\n",
      "# we haven't configured authentication, we pass a dummy value\n",
      "openai_api_key = \"EMPTY\"\n",
      "# modify this value to match your host, remember to add /v1 at the end\n",
      "openai_api_base = \"https://autumn-snow-1380.ploomber.app/v1\"\n",
      "client = OpenAI(\n",
      "  api_key=openai_api_key,\n",
      "  base_url=openai_api_base,\n",
      ")\n",
      "completion = client.completions.create(model=\"google/gemma-2b\",\n",
      "                   prompt=\"JupySQL is\",\n",
      "                   max_tokens=20)\n",
      "print(completion.choices[0].text)\n",
      "\n",
      "```\n",
      "\n",
      "I got the following output:\n",
      "Copy```\n",
      "a powerful SQL editor and IDE. It integrates with Google Jupyter Notebook,\n",
      "which allows users to create and\n",
      "\n",
      "```\n",
      "\n",
      "## Using the chat API\n",
      "The previous example used the completions API; but you might be more familiar with the chat API. Note that if you use the chat API, you must ensure that you use an instruction-tuned model. `google/gemma-2b` **is not** tuned for instructions; letâ€™s instead use `google/gemma-2b-it`, letâ€™s start our vLLM server to use such model:\n",
      "Copy```\n",
      "python -m vllm.entrypoints.openai.api_server \\\n",
      "  --host 0.0.0.0 --port 80 \\\n",
      "  --model google/gemma-2b \\\n",
      "  --dtype=half\n",
      "\n",
      "```\n",
      "\n",
      "Now we can use the `client.chat.completions.create` function:\n",
      "Copy```\n",
      "# NOTE: remember to run: pip install openai\n",
      "from openai import OpenAI\n",
      "openai_api_key = \"EMPTY\"\n",
      "openai_api_base = \"https://autumn-snow-1380.ploomber.app/v1\"\n",
      "client = OpenAI(\n",
      "  api_key=openai_api_key,\n",
      "  base_url=openai_api_base,\n",
      ")\n",
      "chat_response = client.chat.completions.create(\n",
      "  model=\"google/gemma-2b-it\",\n",
      "  messages=[\n",
      "    {\"role\": \"user\", \"content\": \"Tell me in one sentence what Mexico is famous for\"},\n",
      "  ]\n",
      ")\n",
      "print(chat_response.choices[0].message.content)\n",
      "\n",
      "```\n",
      "\n",
      "Output:\n",
      "Copy```\n",
      "Mexico is known for its rich culture, vibrant cities, stunning natural beauty,\n",
      "and delicious cuisine.\n",
      "\n",
      "```\n",
      "\n",
      "Sounds accurate!\n",
      "If youâ€™ve used OpenAIâ€™s API before, you might remember that the `messages` argument usually contains some messages with `{\"role\": \"system\", \"content\": ...}`:\n",
      "Copy```\n",
      "chat_response = client.chat.completions.create(\n",
      "  model=\"google/gemma-2b-it\",\n",
      "  messages=[\n",
      "    {\"role\": \"system\", \"content\": \"You're a helful assistant.\"},\n",
      "    {\"role\": \"user\", \"content\": \"Tell me in one sentence what Mexico is famous for\"},\n",
      "  ]\n",
      "\n",
      "```\n",
      "\n",
      "However, some models do not support the system role, for example, `google/gemma-2b-it` returns the following:\n",
      "Copy```\n",
      "BadRequestError: Error code: 400 - {'object': 'error', 'message': 'System role not\n",
      "supported', 'type': 'BadRequestError', 'param': None, 'code': 400}\n",
      "\n",
      "```\n",
      "\n",
      "Check your modelâ€™s documentation to know how to use the chat API.\n",
      "### Security settings\n",
      "By default, your server wonâ€™t have any authentication. If youâ€™re planning to expose your server to the internet, ensure you set an API key; you can generate one as follows:\n",
      "Copy```\n",
      "export VLLM_API_KEY=$(python -c 'import secrets; print(secrets.token_urlsafe())')\n",
      "# print the API key\n",
      "echo $VLLM_API_KEY\n",
      "\n",
      "```\n",
      "\n",
      "And start vLLM:\n",
      "Copy```\n",
      "python -m vllm.entrypoints.openai.api_server --model google/gemma-2b-it --dtype=half\n",
      "\n",
      "```\n",
      "\n",
      "Now, our server will be protected, and all requests that donâ€™t have the API key will be rejected. Note that in the previous command, we did not pass `--api-key` because vLLM will automatically read the `VLLM_API_KEY` environment variable.\n",
      "Test that your server has API key authentication by making a call using any of the earlier Python snippets, youâ€™ll see the following error:\n",
      "Copy```\n",
      "No key: `AuthenticationError: Error code: 401 - {'error': 'Unauthorized'}`\n",
      "\n",
      "```\n",
      "\n",
      "To fix this, initialize the `OpenAI` client with the correct API key:\n",
      "Copy```\n",
      "from openai import OpenAI\n",
      "openai_api_key = \"THE_ACTUAL_API_KEY\"\n",
      "openai_api_base = \"https://autumn-snow-1380.ploomber.app/v1\"\n",
      "client = OpenAI(\n",
      "  api_key=openai_api_key,\n",
      "  base_url=openai_api_base,\n",
      ")\n",
      "\n",
      "```\n",
      "\n",
      "Another essential security requirement is to serve your API via HTTPS; however, this requires extra configuration, such as getting a TLS certificate. If you want to skip all this headache, [skip to the final section](https://ploomber.io/blog/vllm-deploy/#deploying-on-ploomber-cloud), where weâ€™ll show a one-click solution for securely deploying a vLLM server.\n",
      "## Considerations for a production deployment\n",
      "Here are some considerations for a production deployment:\n",
      "When deploying vLLM, you must ensure that the API restarts if it crashes (or if the physical server is restarted). You can do so with tools such as [`systemd`](https://en.wikipedia.org/wiki/Systemd).\n",
      "To make your deployment more portable, we recommend using `docker` (more in the next section). Also, ensure to pin all Python dependencies so upgrades donâ€™t break your installation (e.g., using `pip freeze`).\n",
      "## Using PyTorchâ€™s docker image\n",
      "We recommend using [PyTorchâ€™s official Docker image](https://hub.docker.com/r/pytorch/pytorch) since it already comes with `torch` and CUDA drivers installed.\n",
      "Hereâ€™s a sample Dockerfile you can use:\n",
      "Copy```\n",
      "FROM pytorch/pytorch:2.1.2-cuda12.1-cudnn8-develWORKDIR /srvRUN pip install vllm==0.3.3 --no-cache-dir# if the model you want to serve requires you to accept the license terms,# you must pass a HF_TOKEN environment variable, also ensure to pass a VLLM_API_KEY# environment variable to authenticate your APIENTRYPOINT [\"python\", \"-m\", \"vllm.entrypoints.openai.api_server\", \\\n",
      "      \"--host\", \"0.0.0.0\", \"--port\", \"80\", \\\n",
      "      \"--model\", \"google/gemma-2b-it\", \\\n",
      "      # depending on your GPU, you might or might not need to pass --dtype      \"--dtype=half\"]\n",
      "```\n",
      "\n",
      "### Cautionary tale about a bug in the `transformers==4.39.1` package\n",
      "tl;dr; when installing vLLM in the official PyTorch docker image, ensure you use the image with the correct PyTorch version. To do so, check the corresponding [`pyproject.toml` file](https://github.com/vllm-project/vllm/blob/10e6322283a9149c23eb76db50e6da972ce4b99e/pyproject.toml#L8)\n",
      "While developing this guide, we encountered a bug in the `transformers` package. We wrote a `Dockerfile` that used the `torch==2.2.2` (the most recent version at this time of writing), and then installed `vllm==0.3.3`:\n",
      "Copy```\n",
      "FROM pytorch/pytorch:2.2.2-cuda12.1-cudnn8-develRUN pip install vllm==0.3.3\n",
      "```\n",
      "\n",
      "However, when starting the vLLM server, we encountered the following error:\n",
      "Copy```\n",
      "File /opt/conda/lib/python3.10/site-packages/transformers/utils/generic.py:478\n",
      "  475   return output_type(**dict(zip(context, values)))\n",
      "  477 if version.parse(get_torch_version()) >= version.parse(\"2.2\"):\n",
      "--> 478   _torch_pytree.register_pytree_node(\n",
      "  479     ModelOutput,\n",
      "  480     _model_output_flatten,\n",
      "  481     partial(_model_output_unflatten, output_type=ModelOutput),\n",
      "  482     serialized_type_name=f\"{ModelOutput.__module__}.{ModelOutput.__name__}\",\n",
      "  483   )\n",
      "  484 else:\n",
      "  485   _torch_pytree._register_pytree_node(\n",
      "  486     ModelOutput,\n",
      "  487     _model_output_flatten,\n",
      "  488     partial(_model_output_unflatten, output_type=ModelOutput),\n",
      "  489   )\n",
      "AttributeError: module 'torch.utils._pytree' has no attribute 'register_pytree_node'\n",
      "\n",
      "```\n",
      "\n",
      "Upon further investigation, we realized that the problem is in the transformers package, specifically, in the [`_is_package_available` function.](https://github.com/huggingface/transformers/blob/a25037beb9f039270b30a94c34ead72ea80ae8a5/src/transformers/utils/import_utils.py#L41). This function determines the current `torch` version, which is used in several parts of the codebase. Even though, vLLM does not use `transformers` for inference, it seems to use it for loading model configuration parameters. The problem that the `transformers` library [uses a method](https://github.com/huggingface/transformers/blob/a25037beb9f039270b30a94c34ead72ea80ae8a5/src/transformers/utils/generic.py#L477) that might return an incorrect version.\n",
      "In our case, the Docker image had `torch==2.2.2`, but since `vllm==0.3.3` requires `pyotrch==2.1.2`, running `pip install vllm==0.3.3` downgraded PyTorch to version 2.1.2, however, `transformers` thought it still had `torch==2.2.2`, crashing execution.\n",
      "This happened with `transformers==4.39.1`, so it might be fixed in future versions.\n",
      "## Deploying on Ploomber Cloud\n",
      "If you want to skip the configuration headache, you can deploy vLLM on Ploomber Cloud with one click. We ensure that:\n",
      "  * All the proper CUDA drivers are installed\n",
      "  * Optimize the hardware vLLM runs on to maximize efficiency\n",
      "  * Provide you with a TLS certificate to serve over HTTPS\n",
      "  * You can stop the server at any time to save costs\n",
      "\n",
      "\n",
      "To learn more, [check our documentation](https://docs.cloud.ploomber.io/en/latest/apps/vllm.html?utm_source=ploomber&utm_medium=blog&utm_campaign=vllm-deploy)\n",
      "Seamless deployment for data scientists and developers. Ploomber handles infrastructure so you focus on building. Secure and scalableâ€”from personal projects to enterprise apps. Support for Streamlit, Dash, Docker, and AI-powered applications. Because life's too short for deployment headaches.\n",
      "### Deploy **a vLLM server in a few clicks** with Ploomber\n",
      "[Start for free](https://www.platform.ploomber.io/register/?utm_source=vllm-deploy&utm_medium=blog) [Explore](https://docs.cloud.ploomber.io/en/latest/intro.html)\n",
      "Share\n",
      "[](https://x.com/share?url=https://ploomber.io/blog/vllm-deploy/&text=Deploying%20vLLM:%20a%20Step-by-Step%20Guide)[](https://www.linkedin.com/shareArticle?mini=true&url=https://ploomber.io/blog/vllm-deploy/&title=Deploying%20vLLM:%20a%20Step-by-Step%20Guide)\n",
      "## Recent Articles\n",
      "[![blog-post](https://ploomber.io/images/blog/streamlit-native-auth-header_hu98e269a0a39c01a47710ee133ce57701_674373_52cf8264e01cf2698fe1b8109741cc56.webp)](https://ploomber.io/blog/streamlit-native-auth/ \"A deep-dive into Streamlit's new authentication capabilities\")\n",
      "### [A deep-dive into Streamlit's new authentication capabilities](https://ploomber.io/blog/streamlit-native-auth/ \"A deep-dive into Streamlit's new authentication capabilities\")\n",
      "![author image](https://ploomber.io/images/author/guillaume_hu117303bbfb75a04885a9d47ea9202532_4407657_ec602000ef8e9818592a97fb67f1b6e1.webp)\n",
      "Guillaume Thibault\n",
      "Feb 06, 2025 - 9 Min read\n",
      "[![blog-post](https://ploomber.io/images/blog/prompt-guard-header_huf2cdc8a9b0e54a6fc7e4d637e890b31c_192734_4306dbaf2c9cb319212455a46bb117d9.webp)](https://ploomber.io/blog/prompt-guard/ \"An introduction to prompt injection with Prompt Guard\")\n",
      "### [An introduction to prompt injection with Prompt Guard](https://ploomber.io/blog/prompt-guard/ \"An introduction to prompt injection with Prompt Guard\")\n",
      "![author image](https://ploomber.io/images/author/eduardo_hu9e19b4a2aaddfa8a3701db9c795afd57_8621_5a035ab53cde9a40f0a262203a70a108.webp)\n",
      "Eduardo Blancas\n",
      "Jan 24, 2025 - 10 Min read\n",
      "This site uses cookies. By continuing to use this website, you agree to their use.I Accept\n",
      "![Ploomber Icon](https://ploomber.io/images/small-icon.png)\n",
      "# Ploomber\n",
      "[Get Started For Free](https://www.platform.ploomber.io/register?utm_medium=website&utm_source=main-button)\n",
      "  * [](https://github.com/ploomber \"Ploomber GitHub\")\n",
      "  * [](https://ploomber.io/community/ \"Ploomber Community Slack\")\n",
      "  * [](https://twitter.com/ploomber \"Ploomber Twitter\")\n",
      "  * [](https://www.linkedin.com/company/ploomber \"Ploomber LinkedIn\")\n",
      "  * [](https://www.youtube.com/channel/UCaIS5BMlmeNQE4-Gn0xTDXQ \"Ploomber Youtube Channel\")\n",
      "  * [](https://share.hsforms.com/1E7Qa_OpcRPi_MV-segFsaAe6c2g \"Ploomber Newsletter\")\n",
      "\n",
      "\n",
      "## Quick Links\n",
      "[Blog](https://ploomber.io/blog/) [Docs](https://docs.cloud.ploomber.io) [AI Editor](https://editor.ploomber.io/) [Convert](https://convert.ploomber.io/) [Pricing](https://ploomber.io/pricing/) [Careers](https://www.ycombinator.com/companies/ploomber/jobs) [Privacy](https://ploomber.io/privacy/) [Terms & Conditions](https://ploomber.io/terms-and-conditions/)\n",
      "## Location & Contacts\n",
      "[+19179089460](tel:+19179089460) contact@ploomber.io69 Charlton Street, New York, NY 10014\n",
      "Ploomber 2025\n",
      "\n",
      "\n",
      "### Response:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[The key findings from the provided documents include the deployment of vLLM as a scalable solution for serving large language models (LLMs) at scale, with Ploomber offering a streamlined deployment process. The guide provides a step-by-step approach to installing and configuring vLLM, emphasizing the importance of prerequisites like CUDA compatibility and proper API key authentication. It also highlights the integration of vLLM with OpenAI and other APIs, showcasing its versatility in providing REST endpoints for model inference. Additionally, the document addresses potential issues, such as the bug in the transformers package, and offers a one-click deployment option on Ploomber Cloud for secure and efficient infrastructure. The focus is on production readiness, including handling crashes and ensuring compatibility with various models and environments. The summary should be concise, capturing the main points without exceeding 512 characters.]\n",
      "The key findings from the provided documents include the deployment of vLLM as a scalable solution for serving large language models (LLMs) at scale, with Ploomber offering a streamlined deployment process. The guide provides a step-by-step approach to installing and configuring vLLM, emphasizing the importance of prerequisites like CUDA compatibility and proper API key authentication. It also highlights the integration of vLLM with OpenAI and other APIs\n"
     ]
    }
   ],
   "source": [
    "_ = generate_text(dataset[11][\"instruction\"], streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d68e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offline-sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
