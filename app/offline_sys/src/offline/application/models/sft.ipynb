{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506f2785",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8e0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764adefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea932b",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26a76e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 40960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bf89efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-30 03:14:30 [__init__.py:243] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.9.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060. Num GPUs = 1. Max memory: 11.638 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4ada4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.5.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=256,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=256,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e3c99",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "483e3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "You are a helpful assistant specialized in summarizing documents. Generate a concise TL;DR summary in markdown format having a maximum of 512 characters of the key findings from the provided documents, highlighting the most significant insights\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"instruction\"]\n",
    "    outputs = examples[\"answer\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
    "\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6760542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tuanlda78202/leo_summarization_task\")\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb614b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3316cfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,  # Can make training 5x faster for short sequences\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=16,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        # warmup_steps = 5,\n",
    "        # max_steps = max_steps,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"sft_outputs\",\n",
    "        report_to=\"comet_ml\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "267236b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3060. Max memory = 11.638 GB.\n",
      "2.4 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c9a6353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 448 | Num Epochs = 1 | Total steps = 7\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 4 x 1) = 64\n",
      " \"-____-\"     Trainable parameters = 278,921,216/7,000,000,000 (3.98% trained)\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/tuanlda78202/leo/495f593d3ce849bba24201b483b09e61\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 04:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.031600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.738800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.648800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.383500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.222500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09e8f1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "295.978 seconds used for training.\n",
      "4.93 minutes used for training.\n",
      "Peak reserved memory = 5.832 GB.\n",
      "Peak reserved memory for training = 3.432 GB.\n",
      "Peak reserved memory % of max memory = 50.112 %.\n",
      "Peak reserved memory for training % of max memory = 29.49 %.\n"
     ]
    }
   ],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime'] / 60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb59a0",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad793a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "You are a helpful assistant specialized in summarizing documents. Generate a concise TL;DR summary in markdown format having a maximum of 512 characters of the key findings from the provided documents, highlighting the most significant insights\n",
      "\n",
      "### Input:\n",
      "# Code\n",
      "\n",
      "# SFT\n",
      "\n",
      "- [https://www.philschmid.de/fine-tune-llms-in-2025](https://www.philschmid.de/fine-tune-llms-in-2025)\n",
      "\t- [https://www.reddit.com/r/LocalLLaMA/comments/1eg0cap/is_lora_finetuning_sometimes_less_effective_than/](https://www.reddit.com/r/LocalLLaMA/comments/1eg0cap/is_lora_finetuning_sometimes_less_effective_than/)\n",
      "\t- [https://www.reddit.com/r/LocalLLaMA/comments/17pw7bv/eternal_question_what_rank_r_and_alpha_to_use_in/](https://www.reddit.com/r/LocalLLaMA/comments/17pw7bv/eternal_question_what_rank_r_and_alpha_to_use_in/)\n",
      "\t- [https://www.youtube.com/watch?v=6l8GZDPbFn8](https://www.youtube.com/watch?v=6l8GZDPbFn8)\n",
      "\t- [https://codecompass00.substack.com/p/qlora-visual-guide-finetune-quantized-llms-peft](https://codecompass00.substack.com/p/qlora-visual-guide-finetune-quantized-llms-peft)\n",
      "\t- [https://news.ycombinator.com/item?id=42085665](https://news.ycombinator.com/item?id=42085665)\n",
      "\n",
      "# RFT\n",
      "\n",
      "- [https://docs.unsloth.ai/get-started/fine-tuning-guide](https://docs.unsloth.ai/get-started/fine-tuning-guide)\n",
      "\t- [https://huggingface.co/docs/trl/sft_trainer](https://huggingface.co/docs/trl/sft_trainer)\n",
      "\t- [https://unfoldai.com/reasoning-in-a-non-english-language/](https://unfoldai.com/reasoning-in-a-non-english-language/)\n",
      "\t- [https://archive.ph/4dzGb#selection-5151.0-5158.0](https://archive.ph/4dzGb#selection-5151.0-5158.0)\n",
      "\n",
      "- [https://www.gilesthomas.com/page/4](https://www.gilesthomas.com/page/4)\n",
      "\t- [https://www.gilesthomas.com/2024/02/llm-quantisation-weirdness](https://www.gilesthomas.com/2024/02/llm-quantisation-weirdness)\n",
      "\t- Series: Messing around with fine-tuning LLMs\n",
      "\t\t- [https://www.gilesthomas.com/2024/04/fine-tuning](https://www.gilesthomas.com/2024/04/fine-tuning)\n",
      "\t\t\t- [https://www.reddit.com/r/LocalLLaMA/comments/18o5u0k/helpful_vram_requirement_table_for_qlora_lora_and/](https://www.reddit.com/r/LocalLLaMA/comments/18o5u0k/helpful_vram_requirement_table_for_qlora_lora_and/)\n",
      "\n",
      "- [https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/overview.html](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/scaling/JAX/overview.html)\n",
      "\t- [https://wandb.ai/byyoung3/ml-news/reports/A-Guide-to-DeepSpeed-Zero-With-the-HuggingFace-Trainer--Vmlldzo2ODkwMDc4](https://wandb.ai/byyoung3/ml-news/reports/A-Guide-to-DeepSpeed-Zero-With-the-HuggingFace-Trainer--Vmlldzo2ODkwMDc4)\n",
      "\n",
      "# Tools\n",
      "\n",
      "- [https://github.com/linkedin/Liger-Kernel](https://github.com/linkedin/Liger-Kernel)\n",
      "- [https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)\n",
      "- [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)\n",
      "- [https://github.com/sgl-project/sglang](https://github.com/sgl-project/sglang)\n",
      "\t- [https://github.com/sgl-project/sgl-learning-materials](https://github.com/sgl-project/sgl-learning-materials)\n",
      "\t- [https://www.youtube.com/watch?v=XQylGyG7yp8](https://www.youtube.com/watch?v=XQylGyG7yp8)\n",
      "\n",
      "- [https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)\n",
      "\t- [https://ploomber.io/blog/vllm-deploy/](https://ploomber.io/blog/vllm-deploy/)\n",
      "\n",
      "- [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)\n",
      "\t- [https://blog.ngxson.com/common-ai-model-formats](https://blog.ngxson.com/common-ai-model-formats)\n",
      "\n",
      "- [https://openrouter.ai/](https://openrouter.ai/)\n",
      "\n",
      "### Response:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TL;DR\n",
      "\n",
      "Fine-tuning LLMs is a critical step in achieving specific tasks. The key findings from the provided documents highlight the importance of choosing the right approach, such as LORA, QLoRA, or full fine-tuning, depending on the task and computational resources. Additionally, the documents emphasize the need for careful calibration of hyperparameters like rank, alpha, and the use of tools like DeepSpeed-Zero and DeepSpeed-LoRA for efficient fine-tuning. The response provides a concise summary of the most significant insights in markdown format, highlighting the most important findings.\n",
      "```markdown\n",
      "# TL;DR\n",
      "\n",
      "Fine-tuning LLMs is a critical step in achieving specific tasks. The key findings from the provided documents highlight the importance of choosing the right approach, such as LORA, QLoRA, or full fine-tuning, depending on the task and computational resources. Additionally, the documents emphasize the need for careful calibration of hyperparameters like rank, alpha, and the use of tools like DeepSpeed-Zero and DeepSpeed-LoRA for efficient fine-tuning.\n",
      "```markdown\n",
      "```python\n",
      "from langchain import LLM\n",
      "from langchain.llms import openai\n",
      "import openai\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.metrics.pairwise import cosine_similarity\n",
      "from sklearn.decomposition import PCA\n",
      "import seaborn as sns\n",
      "from collections import defaultdict\n",
      "import torch\n",
      "from torch import Tensor\n",
      "import torch.nn.functional as F\n",
      "import torch.optim as optim\n",
      "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
      "import torchmetrics\n",
      "from torchmetrics import functional\n",
      "from torchmetrics import classification\n",
      "from torchmetrics.functional import classification\n",
      "import torch\n",
      "from torchvision import transforms\n",
      "from torchvision.datasets import CIFAR10\n",
      "from torchvision import datasets\n",
      "from torchvision import transforms\n",
      "from torchvision import models\n",
      "from torchvision.models import resnet18\n",
      "from torchvision.models import resnet50\n",
      "from torchvision.models import resnet101\n",
      "from torchvision.models import resnet152\n",
      "from torchvision.models import resnet34\n",
      "from torchvision.models import resnet18b\n",
      "from torchvision.models import resnet50b\n",
      "from torchvision.models import resnet101b\n",
      "from torchvision.models import resnet152b\n",
      "from torchvision.models import resnet200b\n",
      "from torchvision.models import resnet50\n",
      "from torchvision.models import resnet18\n",
      "from torchvision.models import resnet18_2\n",
      "from torchvision.models import resnet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[38214,   374,   458,  ...,  1159,   592,  4711]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "max_qwen3_seq_length = 40960\n",
    "max_new_tokens = 512\n",
    "\n",
    "\n",
    "def generate_text(instruction):\n",
    "    message = alpaca_prompt.format(\n",
    "        instruction,\n",
    "        \"\",\n",
    "    )\n",
    "    inputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    return model.generate(\n",
    "        **inputs, streamer=text_streamer, max_new_tokens=max_new_tokens, use_cache=True\n",
    "    )\n",
    "\n",
    "\n",
    "generate_text(dataset[\"validation\"][1][\"instruction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2813da",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e20c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1.35 out of 15.49 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:00<00:00, 59.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You are pushing to hub, but you passed your HF username = tuanlda78202.\n",
      "We shall truncate tuanlda78202/Qwen3-1.7B-Leo-Summarization to Qwen3-1.7B-Leo-Summarization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 2.05 out of 15.49 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:00<00:00, 70.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e82931eaaf4b11a922f2b6e966fd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160438cc79384f439196403403600290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/tuanlda78202/Qwen3-1.7B-Leo-Summarization\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_name = \"Qwen3-1.7B-Leo-Summarization\"\n",
    "\n",
    "# Saving to float16 for VLLM\n",
    "model.save_pretrained_merged(\n",
    "    model_name,\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=os.getenv(\"HUGGINGFACE_ACCESS_TOKEN\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "offline-sys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
